
[TOC]


## to be done

## 20170406

如何一次运行一组python程序

www.talkingdata.com

fiddler web debugger

今天笔试光大：有几点问题发现，

C++和java要熟悉一下，可以通过用C++做网络爬虫。

还有数据结构中的node好像很多都考

## 20170507

-----

some useful notes
### PDF 文件中如何取消超链接
1.Adobe Acrobat Pro或者Acrobat  Reader中菜单编辑/首选项/一般 取消 自动从文本检测URL （Edit-preference-general-creat links form  URLS），即可免除阅读时链接的烦恼。PS:此方法只是在自己电脑的PDF软件上设置不显示，实际没有解决和删除PDF文档本身的问题。一般的连接这样是可以去掉的，但是有的连接做成了背景或水印，再用这个方法就去不掉了。  
2.工具栏——右键——高级编辑——选择超链接的按钮：可以查看每页中的超链接情况，通过键盘delete键一个一个删除  
3.高级-Adobe Acrobat的话可以选择文档处理-删除所有链接    不行话做第4步  
4.文档-背景-删除    不行话去第5步 
5.文档-水印-删除
### win10系统 HDMI接口无输出
升级win10后，HDMI接口无输出,笔记本A卡(amd)，求解决。
到AMD官网下载对应型号显卡完整版驱动就行，系统自带的只是基础驱动.
AMD 显卡驱动程序
### 自己的thinkpad装上sourcecodepro字体
source code pro 是adobe发布的一套开源字体（等宽编程字体，非常适合阅读代码），用来作为代码展示、编写非常的合适。
下载source code pro
ttf文件是我们需要安装的字体
打开ttf文件夹，将后缀名为.ttf的文件copy到C:\windows\fonts目录下，copy完成后，下面会出现source code pro字体，windows字体归类，这就表示安装完成。
pycharm: file--setting-- editor-- colors&fonts--font:primary font:

### vga,dvi,hdmi,dp

****
#### 显示适配器
**显卡**（Video card，Graphics card）全称显示接口卡，又称**显示适配器**，是计算机最基本配置、最重要的配件之一。显卡作为电脑主机里的一个重要组成部分，是电脑进行数模信号转换的设备，承担输出显示图形的任务。显卡接在电脑主板上，它将电脑的数字信号转换成模拟信号让显示器显示出来，同时显卡还是有图像处理能力，可协助CPU工作，提高整体的运行速度。对于从事专业图形设计的人来说显卡非常重要。 民用和军用显卡图形芯片供应商主要包括AMD(超微半导体)和Nvidia(英伟达)2家。
**显卡核心又叫GPU，是图形处理器**，在过于以英特尔为主的CPU市场之外，一直处于一个独立的领域。但是显然与CPU不同的是，显卡并不是用来进行高负荷的大量运算任务，显卡没有动辄四核心或八核心的设计，但是会通过成百上千的显存核心单纯而有效的一遍一遍完成属于自己的任务。GPU的快速发展已经成为了机器学习技术的新标准，比如今年谷歌的云平台究竟采用AMD的FirePro GPU服务。
显示适配器上显示俩个显卡Intel(R) HD Graphics 4000 ；AMD Radeon HD 7670M，请问是独立显卡还是集成。
ntel HD Graphics 4000是集成的核心显卡 ；
AMD Radeon HD 7670M是独立显卡；
#### 显卡分类
- 核芯显卡
核芯显卡是Intel产品新一代图形处理核心，和以往的显卡设计不同，Intel凭借其在处理器制程上的先进工艺以及新的架构设计，将图形核心与处理核心整合在同一块基板上，构成一颗完整的处理器。智能处理器架构这种设计上的整合大大缩减了处理核心、图形核心、内存及内存控制器间的数据周转时间，有效提升处理效能并大幅降低芯片组整体功耗，有助于缩小了核心组件的尺寸，为笔记本、一体机等产品的设计提供了更大选择空间。
- - 核芯显卡的优点：低功耗是核芯显卡的最主要优势，由于新的精简架构及整合设计，核芯显卡对整体能耗的控制更加优异，高效的处理性能大幅缩短了运算时间，进一步缩减了系统平台的能耗。高性能也是它的主要优势：核芯显卡拥有诸多优势技术，可以带来充足的图形处理能力，相较前一代产品其性能的进步十分明显。核芯显卡可支持DX10/DX11、SM4.0、OpenGL2.0、以及全高清Full HD MPEG2/H.264/VC-1格式解码等技术，即将加入的性能动态调节更可大幅提升核芯显卡的处理能力，令其完全满足于普通用户的需求。 
- - 核芯显卡的缺点：配置核芯显卡的CPU通常价格不高，同时低端核显难以胜任大型游戏。
- 集成显卡
集成显卡是将显示芯片、显存及其相关电路都集成在主板上，与其融为一体的元件；集成显卡的显示芯片有单独的，但大部分都集成在主板的北桥芯片中；一些主板集成的显卡也在主板上单独安装了显存，但其容量较小，集成显卡的显示效果与处理性能相对较弱，不能对显卡进行硬件升级，但可以通过CMOS调节频率或刷入新BIOS文件实现软件升级来挖掘显示芯片的潜能。
- - 集成显卡的优点：是功耗低、发热量小、部分集成显卡的性能已经可以媲美入门级的独立显卡，所以不用花费额外的资金购买独立显卡。
- - 集成显卡的缺点：性能相对略低，且固化在主板或CPU上，本身无法更换，如果必须换，就只能换主板。

- 独立显卡
独立显卡是指将显示芯片、显存及其相关电路单独做在一块电路板上，自成一体而作为一块独立的板卡存在，它需占用主板的扩展插槽（ISA、PCI、AGP或PCI-E)。
- - 独立显卡的优点：单独安装有显存，一般不占用系统内存，在技术上也较集成显卡先进得多，但性能肯定不差于集成显卡，容易进行显卡的硬件升级。
- - 独立显卡的缺点：系统功耗有所加大，发热量也较大，需额外花费购买显卡的资金，同时（特别是对笔记本电脑）占用更多空间。
由于显卡性能的不同对于显卡要求也不一样，独立显卡实际分为两类，
一类专门为游戏设计的娱乐显卡，
一类则是用于绘图和3D渲染的专业显卡。

数据（data）一旦离开CPU，必须通过4个步骤，最后才会到达显示屏：
1．从总线（Bus）进入GPU（Graphics Processing Unit，图形处理器）：将CPU送来的数据送到北桥（主桥）再送到GPU（图形处理器）里面进行处理。
2．从 Video Chipset（显卡芯片组）进入 Video RAM（显存）：将芯片处理完的数据送到显存。
3．从显存进入Digital Analog Converter （= RAM DAC，随机读写存储数—模转换器）：从显存读取出数据再送到RAM DAC进行数据转换的工作（数字信号转模拟信号）。但是如果是DVI接口类型的显卡，则不需要经过数字信号转模拟信号。而直接输出数字信号。
4．从DAC进入显示器（Monitor）：将转换完的模拟信号送到显示屏。
显示效能是系统效能的一部分，其效能的高低由以上四步所决定，它与显示卡的效能（Video Performance）不太一样，如要严格区分，显示卡的效能应该受中间两步所决定，因为这两步的资料传输都是在显示卡的内部。第一步是由CPU（运算器和控制器一起组成的计算机的核心，称为微处理器或中央处理器）进入到显示卡里面，最后一步是由显示卡直接送资料到显示屏上。

#### 显卡接口类型：VGA、DVI、HDMI、DP
对于显卡接口类型，主要包括VGA、dvi/HDMI、dp这四种比较常见的接口，当然还有其他的。
**VGA接口**是最常见，也就是我们通常的电脑显示器连接主机的那种，VGA接口是一种D型接口，上面共有15针，分成三排，每排五个。并且VGA接口扩展性比较强，可以轻松与DVI接口进行转换，VGA接口介绍如下图：
![VGA接口](http://opl0hcza3.bkt.clouddn.com/VGA%E6%8E%A5%E5%8F%A3.jpg)
通过上面介绍了VGA接口包括15个针脚，那么15个针脚都代表上面含义呢？功能是什么呢？如下图所示：

![VGA接口管脚](http://opl0hcza3.bkt.clouddn.com/VGA%E6%8E%A5%E5%8F%A3%E7%AE%A1%E8%84%9A.jpg)
目前大多数计算机与外部显示设备之间都是通过模拟VGA接口连接，计算机内部以数字方式生成的显示图像信息，被显卡中的数字/模拟转换器转变为R、G、Ｂ三原色信号和行、场同步信号，信号通过电缆传输到显示设备中。对于模拟显示设备，如模拟CRT显示器，信号被直接送到相应的处理电路，驱动控制显像管生成图像。而对于LCD、DLP等数字显示设备，显示设备中需配置相应的Ａ/Ｄ（模拟/数字）转换器，将模拟信号转变为数字信号。在经过Ｄ/Ａ和Ａ/Ｄ2次转换后，不可避免地造成了一些图像细节的损失。VGA接口应用于CRT显示器无可厚非，但用于连接液晶之类的显示设备，则转换过程的图像损失会使显示效果略微下降。
**DVI接口**：LCD显示器应运而生接口，DVI（Digital Video Interface），即数字视频接口。它是1999年由Silicon Image、Intel（英特尔）、Compaq（康柏）、IBM、HP（惠普）、NEC、Fujitsu(富士通)等公司共同组成DDWG（Digital Display Working Group，数字显示工作组）推出的接口标准。

**HDMI接口：高清输入**
高清晰度多媒体接口是一种数字化视频/音频接口技术，是适合影像传输的专用型数字化接口，其可同时传送音频和影音信号，最高数据传输速度为5Gbps。同时无需在信号传送前进行数/模或者模/数转换。HDMI可搭配宽带数字内容保护（HDCP），以防止具有著作权的影音内容遭到未经授权的复制。HDMI所具备的额外空间可应用在日后升级的音视频格式中。而因为一个1080p的视频和一个8声道的音频信号需求少于4Gbps，因此HDMI还有很大余量。这允许它可以用一个电缆分别连接DVD播放器，接收器和PRR。

HDMI接口的优势HDMI不仅可以满足1080P的分辨率，还能支持DVD Audio等数字音频格式，支持八声道96kHz或立体声192kHz数码音频传送。　　HDMI支持EDID、DDC2B，因此具有HDMI的设备具有“即插即用”的特点，信号源和显示设备之间会自动进行“协商”，自动选择最合适的视频/音频格式。　　与DVI相比HDMI接口的体积更小，DVI的线缆长度不能超过8米，否则将影响画面质量，而HDMI最远可传输15米。只要一条HDMI缆线，就可以取代最多13条模拟传输线，能有效解决家庭娱乐系统背后连线杂乱纠结的问题。
**dp接口**和HDMI接口的区别有哪些？
HDMI是个受HDMI组织和版权严格控制的标准。它不能成为专有的应用，而且所有的数据结构和可用的模块都是被定义好的。从技术角度来说，**DisplayPort** 是更好的，且对于更长和更细的线缆具有良好的鲁棒性。它可以很容易地、自由地降低制程。就计算机市场来说，它的开放性模式和可扩展性特点，对于更高带宽、多数据流和其他数据流是不二的首选。

四种常用的显卡接口就介绍完成了，现在对于显卡显示输入接口的几条选择要点如下：
1、VGA接口由于是模拟信号，不可避免地造成了一些图像细节的损失，只适用于20寸以下的显示器使用；
2、若显示器采用1920*1200以下分辨率，DVI-I，DVI-D，HDMI，DP接口均可，屏幕尺寸影响较小；
3、若显示器采用超高分辨率，比如2560*1600或搭建多屏显示输出系统情况下，双通道DVI-D，HDMI，DP可以胜任；
4、若采用的是3D显示器，1920*1080分辨率下，DVI-D、HDMI和DP没太大问题，若是2560*1600以上最好选用HDMI和DP输出接口。



### CPU，GPU与大数据人工智能
#### GPU为什么能驱动人工智能，以及它是否会被仿
人工智能说到底不外乎将人的五官模仿出来再配合数学公式来计算出下一步的结果
处理五官的信息其实说到底就是不断的进行数学计算，用非人话说就是把采集到的数据以一定的格式呈现，常见的有矩阵，然后再对矩阵进行数学或非数学的加工，最后将加工的结果套用一些数学公式，希望达到人类思考的结果。比方说距离公式
说人话就是通过很复杂的数学计算和数学公式，企图模仿人从收集信息到处理信息最后给出反应一系列的过程
由于信息很多很复杂，传统CPU只有几个核心根本处理不过来，而且都是一些很简单的浮点运算为主，传统CPU根本就是大材小用，所以用GPU会更合适。
通过把信息拆分成N个片段送到GPU，通过多线程，这样会比较快的得出想要的结果，配合CPU，就能加速整个人工智能的过程。
至于会不会被模仿，就好像药厂的配方，除非它开源，否则要模仿也不是容易的事儿。
#### CPU 和 GPU 的区别
CPU和GPU之所以大不相同，是由于其设计目标的不同，它们分别针对了两种不同的应用场景。CPU需要很强的通用性来处理各种不同的数据类型，同时又要逻辑判断又会引入大量的分支跳转和中断的处理。这些都使得CPU的内部结构异常复杂。而GPU面对的则是类型高度统一的、相互无依赖的大规模数据和不需要被打断的纯净的计算环境。
　　于是CPU和GPU就呈现出非常不同的架构（示意图）：
  ![cpugpu1](C:\Users\sswsd\Desktop\privateNotes\privateNotesPictures\cpugpu1.jpg)
  ![cpugpu1](http://opl0hcza3.bkt.clouddn.com/cpugpu1.jpg)

图片来自nVidia CUDA文档。其中绿色的是计算单元，橙红色的是存储单元，橙黄色的是控制单元。
从上图可以看出：
Cache, local memory： CPU > GPU 
Threads(线程数): GPU > CPU
Registers: GPU > CPU  多寄存器可以支持非常多的Thread,thread需要用到register,thread数目大，register也必须得跟着很大才行。

SIMD Unit(单指令多数据流,以同步方式，在同一时间内执行同一条指令): GPU > CPU。 
CPU 基于低延时的设计：
![cpugpu2](http://opl0hcza3.bkt.clouddn.com/cpugpu2.png)
CPU有强大的ALU（算术运算单元）,它可以在很少的时钟周期内完成算术计算。
当今的CPU可以达到64bit 双精度。执行双精度浮点源算的加法和乘法只需要1～3个时钟周期。
CPU的时钟周期的频率是非常高的，达到1.532～3gigahertz(千兆HZ, 10的9次方).
大的缓存也可以降低延时。保存很多的数据放在缓存里面，当需要访问的这些数据，只要在之前访问过的，如今直接在缓存里面取即可。
复杂的逻辑控制单元。当程序含有多个分支的时候，它通过提供分支预测的能力来降低延时。

数据转发。 当一些指令依赖前面的指令结果时，数据转发的逻辑控制单元决定这些指令在pipeline中的位置并且尽可能快的转发一个指令的结果给后续的指令。这些动作需要很多的对比电路单元和转发电路单元。

![cpugpu3](http://opl0hcza3.bkt.clouddn.com/cpugpu3.png)
GPU是基于大的吞吐量设计。

GPU的特点是有很多的ALU和很少的cache. 缓存的目的不是保存后面需要访问的数据的，这点和CPU不同，而是为thread提高服务的。如果有很多线程需要访问同一个相同的数据，缓存会合并这些访问，然后再去访问dram（因为需要访问的数据保存在dram中而不是cache里面），获取数据后cache会转发这个数据给对应的线程，这个时候是数据转发的角色。但是由于需要访问dram，自然会带来延时的问题。

GPU的控制单元（左边黄色区域块）可以把多个的访问合并成少的访问。

GPU的虽然有dram延时，却有非常多的ALU和非常多的thread. 为啦平衡内存延时的问题，我们可以中充分利用多的ALU的特性达到一个非常大的吞吐量的效果。尽可能多的分配多的Threads.通常来看GPU ALU会有非常重的pipeline就是因为这样。


  所以与CPU擅长逻辑控制，串行的运算。和通用类型数据运算不同，GPU擅长的是大规模并发计算，这也正是密码破解等所需要的。所以GPU除了图像处理，也越来越多的参与到计算当中来。

  GPU的工作大部分就是这样，计算量大，但没什么技术含量，而且要重复很多很多次。就像你有个工作需要算几亿次一百以内加减乘除一样，最好的办法就是雇上几十个小学生一起算，一人算一部分，反正这些计算也没什么技术含量，纯粹体力活而已。
  而CPU就像老教授，积分微分都会算，就是工资高，一个老教授资顶二十个小学生，你要是富士康你雇哪个？GPU就是这样，用很多简单的计算单元去完成大量的计算任务，纯粹的人海战术。这种策略基于一个前提，就是小学生A和小学生B的工作没有什么依赖性，是互相独立的。很多涉及到大量计算的问题基本都有这种特性，比如你说的破解密码，挖矿和很多图形学的计算。这些计算可以分解为多个相同的简单小任务，每个任务就可以分给一个小学生去做。但还有一些任务涉及到“流”的问题。比如你去相亲，双方看着顺眼才能继续发展。总不能你这边还没见面呢，那边找人把证都给领了。这种比较复杂的问题都是CPU来做的。

　总而言之，CPU和GPU因为最初用来处理的任务就不同，所以设计上有不小的区别。而某些任务和GPU最初用来解决的问题比较相似，所以用GPU来算了。GPU的运算速度取决于雇了多少小学生，CPU的运算速度取决于请了多么厉害的教授。教授处理复杂任务的能力是碾压小学生的，但是对于没那么复杂的任务，还是顶不住人多。当然现在的GPU也能做一些稍微复杂的工作了，相当于升级成初中生高中生的水平。但还需要CPU来把数据喂到嘴边才能开始干活，究竟还是靠CPU来管的。

什么类型的程序适合在GPU上运行？
（1）计算密集型的程序。所谓计算密集型(Compute-intensive)的程序，就是其大部分运行时间花在了寄存器运算上，寄存器的速度和处理器的速度相当，从寄存器读写数据几乎没有延时。可以做一下对比，读内存的延迟大概是几百个时钟周期；读硬盘的速度就不说了，即便是SSD, 也实在是太慢了。
（2）易于并行的程序。GPU其实是一种SIMD(Single Instruction Multiple Data)架构， 他有成百上千个核，每一个核在同一时间最好能做同样的事情。


首先需要解释CPU和GPU这两个缩写分别代表什么。
CPU即中央处理器，
GPU即图形处理器。
其次，要解释两者的区别，要先明白两者的相同之处：两者都有总线和外界联系，有自己的缓存体系，以及数字和逻辑运算单元。
一句话，两者都为了完成计算任务而设计。
两者的区别在于存在于片内的缓存体系和数字逻辑运算单元的结构差异：CPU虽然有多核，但总数没有超过两位数，每个核都有足够大的缓存和足够多的数字和逻辑运算单元，并辅助有很多加速分支判断甚至更复杂的逻辑判断的硬件；** GPU的核数远超CPU **GPU的核数远超CPU，被称为众核（NVIDIA Fermi有512个核）。每个核拥有的缓存大小相对小，数字逻辑运算单元也少而简单（GPU初始时在浮点计算上一直弱于CPU）。从结果上导致CPU擅长处理具有复杂计算步骤和复杂数据依赖的计算任务，如分布式计算，数据压缩，人工智能，物理模拟，以及其他很多很多计算任务等。GPU由于历史原因，是为了视频游戏而产生的（至今其主要驱动力还是不断增长的视频游戏市场），在三维游戏中常常出现的一类操作是对海量数据进行相同的操作，如：对每一个顶点进行同样的坐标变换，对每一个顶点按照同样的光照模型计算颜色值。GPU的众核架构非常适合把同样的指令流并行发送到众核上，采用不同的输入数据执行。
在2003-2004年左右，图形学之外的领域专家开始注意到GPU与众不同的计算能力，开始尝试把GPU用于通用计算（即GPGPU）。之后NVIDIA发布了CUDA，AMD和Apple等公司也发布了OpenCL，GPU开始在通用计算领域得到广泛应用，包括：数值分析，海量数据处理（排序，Map-Reduce等），金融分析等等。简而言之，当程序员为CPU编写程序时，他们倾向于利用复杂的逻辑结构优化算法从而减少计算任务的运行时间，即Latency。当程序员为GPU编写程序时，则利用其处理海量数据的优势，通过提高总的数据吞吐量（Throughput）来掩盖Lantency。目前，CPU和GPU的区别正在逐渐缩小，因为GPU也在处理不规则任务和线程间通信方面有了长足的进步。另外，功耗问题对于GPU比CPU更严重。总的来讲，GPU和CPU的区别是个很大的话题，甚至可以花一个学期用32个学时十几次讲座来讲，所以如果提问者有更具体的问题，可以进一步提出。我会在我的知识范围内尝试回答。





